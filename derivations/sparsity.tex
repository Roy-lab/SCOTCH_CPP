\documentclass{article}
\usepackage{enumerate}
\usepackage{ulem}
\usepackage[left=1in, right=1in, top=1in, bottom=1in,headsep=0pt]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{url}
%\usepackage{booktabs}
%\usepackage{longtable}
\usepackage[english]{babel}
%\renewcommand{\familydefault}{\sfdefault}
\usepackage{subcaption}
\usepackage{amsfonts}
%\usepackage{qtree}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\usepackage{fontspec}
\usepackage{titlesec}
\usepackage[font=footnotesize,labelfont=bf,skip=-10pt]{caption}
\usepackage[super,comma,compress]{natbib}
\setlength{\bibsep}{1pt plus 10ex}

%\setmainfont[Ligatures=TeX]{Arial}
\setlength{\parindent}{0pt}
\setlength{\parskip}{2pt}
\expandafter\def\expandafter\normalsize\expandafter{%
    \normalsize
    \setlength\abovedisplayskip{2pt}
    \setlength\belowdisplayskip{2pt}
    \setlength\abovedisplayshortskip{2pt}
    \setlength\belowdisplayshortskip{2pt}
}

\titleformat{\section}{\normalfont\fontsize{12}{12}\bfseries}{}{}{}%{\MakeUppercase}
\titleformat{\subsection}{\normalfont\bfseries}{12}{12}{}
\titleformat{\subsubsection}[runin]{\normalfont\bfseries}{}{}{}
\titlespacing*{\section}{0pt}{0ex}{0ex}
\titlespacing*{\subsection}{0pt}{0ex}{0ex}
\titlespacing*{\subsubsection}{0pt}{0ex}{5pt}

\DeclareMathOperator{\Tr}{Tr}

\begin{document}
%\maketitle
\begin{center}
\textbf{\large{Deriving the Block Coordinate Descent Rules for\\(single-task) NMF with sparsity regularization}}
\end{center}
\smallskip
\section*{Objective}
Given an input matrix $X \in \mathbb{R}_{\geq 0}^{n \times m}$ and $k \ll n,m$, the objective is to find $U \in \mathbb{R}_{\geq 0}^{n \times k}$, $V \in \mathbb{R}_{\geq 0}^{m \times k}$ that minimizes:
\begin{align}
O &= 
\left\Vert X - U V^\top \right\Vert_\text{F}^2 
+  \lambda \sum_{i=1}^m \left\Vert V [i,:] \right\Vert_1
\end{align} 

where $V[i,:]$ is the $i$ row of factor matrix $V$. The regularization term involving $\lambda$ tries to enforce sparsity in each row of $V$, ultimately so that only one latent dimension ``lights up" for each row of $V$. Higher $\lambda$ will enforce stricter sparsity.

\medskip
\section*{Breaking down to task-level and column-level subproblems}

The objective is equivalent to minimizing:
\begin{align}
O &= 
\left\Vert X - \sum_{j=1}^k u_{j} v^{\top}_{j} \right\Vert_\text{F}^2 
+\lambda \sum_{i=1}^m \sum_{j=1}^k \left\lvert V[i,j] \right\rvert \\
&=
\left\Vert X - \sum_{j=1}^k u_{j} v^{\top}_{j} \right\Vert_\text{F}^2 
+\lambda \sum_{j=1}^k \sum_{i=1}^m \left\lvert V[i,j] \right\rvert \\
&=
\left\Vert X - \sum_{j=1}^k u_{j} v^{\top}_{j} \right\Vert_\text{F}^2 
+\lambda \sum_{j=1}^k \left\Vert v_j \right\Vert_1
\end{align}

Where $u_{j} \in \mathbb{R}^{n}_{\geq 0}$ is the $j$th column vector of $U$, i.e. $U[:,j]$, and  $v_{j}  \in \mathbb{R}^m_{\geq 0}$ is the $j$th column vector of $V$, i.e. $V [:,j]$. Now we `pull out' terms involving the $j$th column:
\begin{align}
O &= 
\left\Vert X -u_{j} v^{\top}_{j} - \sum_{l \neq j} u_{l} v^{\top}_{l} \right\Vert_\text{F}^2 
+\lambda\left\Vert v_j \right\Vert_1
+\lambda \sum_{l \neq j} \left\Vert v_l \right\Vert_1
\end{align}

Now we'll substitute with $R_j = X - \sum_{l\neq j} u_{l} v^{\top}_{l}$:
\begin{align}
O &= 
\left\Vert R_j -u_{j} v^{\top}_{j}  \right\Vert_\text{F}^2 
+\lambda\left\Vert v_j \right\Vert_1
+\lambda \sum_{l \neq j} \left\Vert v_l \right\Vert_1
\end{align}

We can now attempt to optimize $u_{j}$ and $v_{j}$, fixing all other parameters to be constant. 

\medskip
\section*{Optimize $v_{j}$}
To find $v_{j}$ that minimizes the objective, we find the derivative of the objective with respect to $v_{j}$ and set it to 0, then solve. First we expand the objective into matrix multiplications:
\begin{align}
O &= 
\left\Vert R_j -u_{j} v^{\top}_{j}  \right\Vert_\text{F}^2 
+\lambda\left\Vert v_j \right\Vert_1
+ C =
\Tr \left[
\left( R_j -u_{j} v^{\top}_{j} \right)^\top \left( R_j -u_{j} v^{\top}_{j} \right) \right]
+\lambda \left\Vert v_j \right\Vert_1
+ C
\end{align}

Here $C$ subsumes all elements of the objective that does not involve $v_j$, since they will be zeroed out when the derivative is taken with respect to $v_j$. Now we keep expanding:
\begin{align}
O &= 
\Tr \left[
R_j^\top R_j 
- 2R_j ^\top  u_j v_j^\top
+ \left( u_j v_j^\top \right)^\top \left( u_j v_j^\top \right) 
\right] 
+\lambda \left\Vert v_j \right\Vert_1
+ C \\
&= \Tr \left( R^\top_j R_j \right)
- 2 \Tr \left( R^\top_j u_{j} v^\top_{j} \right)
+  \Tr \left( v_{j}  u^{\top}_{j}   u_{j} v^{\top}_{j} \right)
+\lambda \left\Vert v_j \right\Vert_1
+ C \\
&= \Tr \left( R^{^\top}_j R_j \right)
- 2 \left( R^{\top}_j u_{j}  \right)^\top v_{j} 
+  \left( u^{\top}_{j}   u_{j} \right)\left( v^{\top}_{j} v_{j}  \right) 
+\lambda \left\Vert v_j \right\Vert_1
+ C \label{expanded} \\
&= \Tr \left( R^{^\top}_j R_j \right)
- 2 \left( R^{\top}_j u_{j}  \right)^\top v_{j} 
+  \left( u^{\top}_{j}   u_{j} \right)\left( v^{\top}_{j} v_{j}  \right) 
+\lambda \boldsymbol{1}_m^\top v_j
+ C \label{trick}
\end{align}

where $\boldsymbol{1}_m$ is a vector of size $m$, filled with 1's. What allows us to expand $ \left\Vert v_j \right\Vert_1$ from (\ref{expanded}) to $\boldsymbol{1}_m^\top v_j$ (\ref{trick}) is the fact that we're enforcing $v_j$ to be non-negative at initialization and at each iteration.

Now the fun part:
\begin{align}
\frac{\partial O}{\partial v_j} = 0 &=
0 - 2R^{\top}_j u_{j} + 2  v_{j} u^{\top}_{j} u_{j}
+ \lambda \boldsymbol{1}_m
+ 0 \\
&= -R^{\top}_j u_{j} 
+ u^{\top}_{j} u_{j}  v_{j}
+ \frac{\lambda}{2} \boldsymbol{1}_m
\\
v_{j} &= \frac{R^{\top}_j u_{j} - \frac{\lambda}{2} \boldsymbol{1}_m}{\left\Vert u_{j} \right\Vert_2^2}
\end{align}

With the non-negativity constraint $v_{j} \geq 0$, we want $R^{\top}_j u_{j} - \frac{\lambda}{2} \boldsymbol{1}_m \geq 0$, because if $R^{\top}_j u_{j} - \frac{\lambda}{2} \boldsymbol{1}_m < 0$, $O$ will increase in (\ref{trick}). So the finalized update rule is:
\begin{align}
v_{j} &=\frac{ \left[ R^{\top}_j u_{j} - \frac{\lambda}{2} \boldsymbol{1}_m \right]_+}{\left\Vert u_{j} \right\Vert_2^2}
\end{align}

\section*{Optimize $u_{j}$}
We can derive the update rule for $u_{j}$ more simply.  From (\ref{expanded}), we take the derivative of $O$ with respect to $u_j$; all regularization terms will zero out since they do not involve $u_j$. Hence the final update rule for $u_j$ is:
\begin{align}
u_j&= \frac{\left[ R_j v_j \right]_+ }{\left\Vert v_j \right\Vert_2^2 }
\end{align}

\end{document}